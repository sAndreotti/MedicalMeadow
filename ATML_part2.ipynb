{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sAndreotti/MedicalMeadow/blob/main/ATML_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29nxqgohSsQ3"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2eUNhk9tprU"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:58:25.513090Z",
          "iopub.status.busy": "2024-12-30T16:58:25.512794Z",
          "iopub.status.idle": "2024-12-30T16:59:13.506495Z",
          "shell.execute_reply": "2024-12-30T16:59:13.505543Z",
          "shell.execute_reply.started": "2024-12-30T16:58:25.513052Z"
        },
        "id": "SAho3HGib9-U",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# captures the stdout/stderr of a cell. With this magic you can discard these streams or store them in a variable\n",
        "!pip install datasets accelerate peft transformers trl==0.12.0 plotly huggingface_hub\n",
        "!pip install --upgrade smart_open\n",
        "!pip install --upgrade gensim\n",
        "!pip install ffmpeg-python\n",
        "!pip install -U openai-whisper\n",
        "!pip install scipy librosa unidecode inflect\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "!pip install TTS\n",
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes\n",
        "!pip install nltk\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b63DJghtprV"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:13.507795Z",
          "iopub.status.busy": "2024-12-30T16:59:13.507511Z",
          "iopub.status.idle": "2024-12-30T16:59:49.667726Z",
          "shell.execute_reply": "2024-12-30T16:59:49.666822Z",
          "shell.execute_reply.started": "2024-12-30T16:59:13.507760Z"
        },
        "id": "q_JimYqjjY4S",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from trl import SFTTrainer\n",
        "import re\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import plotly.express as px\n",
        "import random\n",
        "from sklearn.manifold import TSNE\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    AutoModelForSpeechSeq2Seq,\n",
        "    AutoProcessor,\n",
        "    pipeline,\n",
        "    TextStreamer\n",
        ")\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tabulate import tabulate\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "from TTS.api import TTS\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2DDMWKGSsQ5"
      },
      "source": [
        "### Import for audio section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkjSoQT4SsQ6"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import scipy\n",
        "import whisper\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcC-cug1tprW"
      },
      "source": [
        "## Hugging Face settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40cASCEmtprW"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "login(token=os.environ.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKL46PcIc21t"
      },
      "source": [
        "## Investigate Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:49.669293Z",
          "iopub.status.busy": "2024-12-30T16:59:49.668679Z",
          "iopub.status.idle": "2024-12-30T16:59:52.220997Z",
          "shell.execute_reply": "2024-12-30T16:59:52.220340Z",
          "shell.execute_reply.started": "2024-12-30T16:59:49.669269Z"
        },
        "id": "YltBm7i4b0IM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
        "ds = ds['train']\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.223297Z",
          "iopub.status.busy": "2024-12-30T16:59:52.223085Z",
          "iopub.status.idle": "2024-12-30T16:59:52.425188Z",
          "shell.execute_reply": "2024-12-30T16:59:52.424496Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.223279Z"
        },
        "id": "n89RynbpcHQB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(ds.features, \"\\n\")\n",
        "print(\"Instruction:\")\n",
        "print(f\"length: {len(ds['instruction'])}\")\n",
        "print(f\"example: {ds['instruction'][0]} \\n\")\n",
        "\n",
        "print(f\"Input:\")\n",
        "print(f\"length: {len(ds['input'])}\")\n",
        "print(f\"example: {ds['input'][0]} \\n\")\n",
        "\n",
        "print(f\"Output:\")\n",
        "print(f\"length: {len(ds['output'])}\")\n",
        "print(f\"example: {ds['output'][0]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9SXu1UFoE4x"
      },
      "source": [
        "### Some plots about the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.426773Z",
          "iopub.status.busy": "2024-12-30T16:59:52.426511Z",
          "iopub.status.idle": "2024-12-30T16:59:52.525435Z",
          "shell.execute_reply": "2024-12-30T16:59:52.524558Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.426751Z"
        },
        "id": "nLEtHKtISsQ7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "instructions = ds['instruction']\n",
        "input_phrases = ds['input']\n",
        "output_phrases = ds['output']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.526665Z",
          "iopub.status.busy": "2024-12-30T16:59:52.526382Z",
          "iopub.status.idle": "2024-12-30T16:59:52.538736Z",
          "shell.execute_reply": "2024-12-30T16:59:52.537936Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.526643Z"
        },
        "id": "4TAMV5DdnRg7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Count the frequency of each unique instruction\n",
        "instruction_counts = {instruction: instructions.count(instruction) for instruction in set(instructions)}\n",
        "\n",
        "# Sort the instructions by frequency\n",
        "sorted_instructions = sorted(instruction_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Separate the instructions and their counts for plotting\n",
        "sorted_instruction_names = [item[0] for item in sorted_instructions]\n",
        "sorted_instruction_counts = [item[1] for item in sorted_instructions]\n",
        "\n",
        "# Plotting the frequency of instructions\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bars = plt.barh(sorted_instruction_names, sorted_instruction_counts, color='skyblue', edgecolor='black', linewidth=1.2)\n",
        "plt.title('Instruction Frequency Distribution')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Instruction')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.539857Z",
          "iopub.status.busy": "2024-12-30T16:59:52.539641Z",
          "iopub.status.idle": "2024-12-30T16:59:52.554118Z",
          "shell.execute_reply": "2024-12-30T16:59:52.553309Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.539829Z"
        },
        "id": "d9lEMELsiqIx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calculate the length of each phrase\n",
        "input_lengths = [len(phrase) for phrase in input_phrases]\n",
        "output_lengths = [len(phrase) for phrase in output_phrases]\n",
        "\n",
        "# Define the bins for the length ranges\n",
        "max_input = max(input_lengths)\n",
        "max_output = max(output_lengths)\n",
        "\n",
        "input_bins = [i * max_input / 10 for i in range(1, 11)]\n",
        "output_bins = [i * max_output / 10 for i in range(1, 11)]\n",
        "bin_labels_input = [f'{int(input_bins[i-1])}-{int(input_bins[i])}' for i in range(1, 10)]\n",
        "bin_labels_output = [f'{int(output_bins[i-1])}-{int(output_bins[i])}' for i in range(1, 10)]\n",
        "\n",
        "# Bin the lengths into the categories\n",
        "input_binned = np.digitize(input_lengths, input_bins)  # Categorize based on input lengths\n",
        "output_binned = np.digitize(output_lengths, output_bins)  # Categorize based on output lengths\n",
        "\n",
        "# Count how many phrases fall into each bin\n",
        "input_bin_counts = [sum(input_binned == i) for i in range(1, len(input_bins))]\n",
        "output_bin_counts = [sum(output_binned == i) for i in range(1, len(output_bins))]\n",
        "\n",
        "# Plotting the bar charts\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Plotting the input phrase lengths\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(bin_labels_input, input_bin_counts, color='skyblue', edgecolor='black')\n",
        "plt.title('Input Phrases Length Distribution')\n",
        "plt.xlabel('Length Range')\n",
        "plt.ylabel('Number of Phrases')\n",
        "\n",
        "# Plotting the output phrase lengths\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(bin_labels_output, output_bin_counts, color='skyblue', edgecolor='black')\n",
        "plt.title('Output Phrases Length Distribution')\n",
        "plt.xlabel('Length Range')\n",
        "plt.ylabel('Number of Phrases')\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJljpW-Jc37a"
      },
      "source": [
        "### Extra Plot\n",
        "WordCloud to show the most frequent words in input and output set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5PVKri8c37a"
      },
      "outputs": [],
      "source": [
        "# Join together all words from input set\n",
        "input = ' '.join(input_phrases)\n",
        "\n",
        "# Join together all words from output set\n",
        "output = ' '.join(output_phrases)\n",
        "\n",
        "# Instantiate WC for input and for output\n",
        "wordcloud1 = WordCloud(width=800, height=400, background_color='white').generate(input)\n",
        "wordcloud2 = WordCloud(width=800, height=400, background_color='white').generate(output)\n",
        "\n",
        "# Plotting the bar charts\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Plotting the input words\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(wordcloud1, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Question Word Cloud')\n",
        "\n",
        "# Plotting the output words\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(wordcloud2, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Answer Word Cloud')\n",
        "\n",
        "# Show\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4wIndWS2lyy"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUU3TXC5c37a"
      },
      "outputs": [],
      "source": [
        "# Tokenize with spaces or any non word character\n",
        "tokenized_sentences = [re.sub('\\W', ' ', sentence).lower().split() for sentence in input_phrases]\n",
        "\n",
        "# remove sentences that are only 1 word long\n",
        "tokenized_sentences = [sentence for sentence in tokenized_sentences if len(sentence) > 1]\n",
        "\n",
        "for sentence in tokenized_sentences[:5]:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.841003Z",
          "iopub.status.busy": "2024-12-30T16:59:52.840758Z",
          "iopub.status.idle": "2024-12-30T16:59:52.851850Z",
          "shell.execute_reply": "2024-12-30T16:59:52.851270Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.840983Z"
        },
        "id": "e05BkgN-2lyz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create Word2Vec (ignore words with frequency less than 5)\n",
        "model = Word2Vec(tokenized_sentences, vector_size=30, min_count=5, window=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.852897Z",
          "iopub.status.busy": "2024-12-30T16:59:52.852659Z",
          "iopub.status.idle": "2024-12-30T16:59:52.864549Z",
          "shell.execute_reply": "2024-12-30T16:59:52.863727Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.852865Z"
        },
        "id": "blQAYdGr2lyz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sample = random.sample(list(model.wv.key_to_index), 500)\n",
        "word_vectors = model.wv[sample]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CskZCC7a2lyz"
      },
      "source": [
        "### 3D plot with words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.867538Z",
          "iopub.status.busy": "2024-12-30T16:59:52.867355Z",
          "iopub.status.idle": "2024-12-30T16:59:52.876777Z",
          "shell.execute_reply": "2024-12-30T16:59:52.876193Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.867521Z"
        },
        "id": "_0EGqbEA2lyz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Apply t-SNE to word vectors\n",
        "tsne = TSNE(n_components=3, n_iter=2000)\n",
        "tsne_embedding = tsne.fit_transform(word_vectors)\n",
        "\n",
        "# Extract individual dimensions\n",
        "x, y, z = np.transpose(tsne_embedding)\n",
        "\n",
        "# Create 3D scatter plot\n",
        "fig = px.scatter_3d(x=x[:200],y=y[:200],z=z[:200],text=sample[:200])\n",
        "fig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.878683Z",
          "iopub.status.busy": "2024-12-30T16:59:52.878417Z",
          "iopub.status.idle": "2024-12-30T16:59:52.889273Z",
          "shell.execute_reply": "2024-12-30T16:59:52.888545Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.878662Z"
        },
        "id": "ruxbKZBt2lyz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "first_question = ['man', 'woman']\n",
        "#question = ['rem', 'sleep', 'hallucinations', 'paralysis']\n",
        "\n",
        "word_vectors = model.wv[first_question+sample]\n",
        "\n",
        "tsne = TSNE(n_components=3)\n",
        "tsne_embedding = tsne.fit_transform(word_vectors)\n",
        "\n",
        "x, y, z = np.transpose(tsne_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.890313Z",
          "iopub.status.busy": "2024-12-30T16:59:52.890087Z",
          "iopub.status.idle": "2024-12-30T16:59:52.902336Z",
          "shell.execute_reply": "2024-12-30T16:59:52.901559Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.890293Z"
        },
        "id": "Cem4IiRb2lyz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "r = (-20,20)\n",
        "fig = px.scatter_3d(x=x, y=y, z=z, range_x=r, range_y=r, range_z=r, text=first_question + [None] * 500)\n",
        "fig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.903428Z",
          "iopub.status.busy": "2024-12-30T16:59:52.903158Z",
          "iopub.status.idle": "2024-12-30T16:59:52.915056Z",
          "shell.execute_reply": "2024-12-30T16:59:52.914368Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.903400Z"
        },
        "id": "E795I6ej2lyz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Use Word2Vec to find most similar words\n",
        "model.wv.most_similar('menopause')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.916083Z",
          "iopub.status.busy": "2024-12-30T16:59:52.915796Z",
          "iopub.status.idle": "2024-12-30T16:59:52.927087Z",
          "shell.execute_reply": "2024-12-30T16:59:52.926279Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.916062Z"
        },
        "id": "8RkQLdpv2lyz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# King – Man + Woman = Queen example\n",
        "vec = model.wv.get_vector('headache') + (model.wv.get_vector('fever') - model.wv.get_vector('drug'))\n",
        "model.wv.similar_by_vector(vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t__NU-CxtprY"
      },
      "source": [
        "## Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etwCQ4z6tprY"
      },
      "outputs": [],
      "source": [
        "class MedDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.dataset[idx]\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": example['instruction']},\n",
        "            {\"role\": \"user\", \"content\": example['input']},\n",
        "            {\"role\": \"assistant\", \"content\": example['output']}\n",
        "        ]\n",
        "\n",
        "        prompt = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        tokens['labels'] = tokens['input_ids'].clone()\n",
        "        tokens['labels'][tokens['input_ids'] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": tokens['input_ids'].squeeze(),\n",
        "            \"attention_mask\": tokens['attention_mask'].squeeze(),\n",
        "            \"labels\": tokens['labels'].squeeze()\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BMh1vbuc5qp"
      },
      "source": [
        "## Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:52.952410Z",
          "iopub.status.busy": "2024-12-30T16:59:52.952160Z",
          "iopub.status.idle": "2024-12-30T16:59:55.064154Z",
          "shell.execute_reply": "2024-12-30T16:59:55.063424Z",
          "shell.execute_reply.started": "2024-12-30T16:59:52.952378Z"
        },
        "id": "AelKMq1xSsQ-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Base model for fine-tuning\n",
        "base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:55.129451Z",
          "iopub.status.busy": "2024-12-30T16:59:55.129104Z",
          "iopub.status.idle": "2024-12-30T16:59:55.142171Z",
          "shell.execute_reply": "2024-12-30T16:59:55.141484Z",
          "shell.execute_reply.started": "2024-12-30T16:59:55.129421Z"
        },
        "id": "RRneCjBj9Lag",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Split dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(ds, [0.8, 0.1, 0.1])\n",
        "\n",
        "train_dataset = MedDataset(train_dataset, tokenizer)\n",
        "val_dataset = MedDataset(val_dataset, tokenizer)\n",
        "\n",
        "print(f\"Train dataset dimension: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset dimension: {len(val_dataset)}\")\n",
        "print(f\"Test dataset dimension: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T16:59:55.142987Z",
          "iopub.status.busy": "2024-12-30T16:59:55.142794Z",
          "iopub.status.idle": "2024-12-30T17:00:59.593527Z",
          "shell.execute_reply": "2024-12-30T17:00:59.592693Z",
          "shell.execute_reply.started": "2024-12-30T16:59:55.142969Z"
        },
        "id": "gR4BtF8AHD0I",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_representation=\"nested\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0},\n",
        "    torch_dtype=torch.float32,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T17:00:59.601461Z",
          "iopub.status.busy": "2024-12-30T17:00:59.601147Z",
          "iopub.status.idle": "2024-12-30T17:00:59.614772Z",
          "shell.execute_reply": "2024-12-30T17:00:59.614099Z",
          "shell.execute_reply.started": "2024-12-30T17:00:59.601433Z"
        },
        "id": "ew98HuwaKW1P",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "peft_params = LoraConfig(\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T17:00:59.615833Z",
          "iopub.status.busy": "2024-12-30T17:00:59.615586Z",
          "iopub.status.idle": "2024-12-30T17:00:59.653074Z",
          "shell.execute_reply": "2024-12-30T17:00:59.652283Z",
          "shell.execute_reply.started": "2024-12-30T17:00:59.615814Z"
        },
        "id": "n8dBHX-j-M1J",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "training_params = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    eval_strategy=\"steps\",\n",
        "    logging_steps=90,\n",
        "    eval_steps = 90,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\",\n",
        "    gradient_checkpointing=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T17:00:59.654307Z",
          "iopub.status.busy": "2024-12-30T17:00:59.654001Z",
          "iopub.status.idle": "2024-12-30T18:27:02.782730Z",
          "shell.execute_reply": "2024-12-30T18:27:02.781673Z",
          "shell.execute_reply.started": "2024-12-30T17:00:59.654278Z"
        },
        "id": "ARnpWrEFSsQ_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=peft_params,\n",
        "    max_seq_length=256,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_params,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "trainer.save_model(\"./fine-tuned-model\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T18:40:06.102654Z",
          "iopub.status.busy": "2024-12-30T18:40:06.102303Z",
          "iopub.status.idle": "2024-12-30T18:40:06.608165Z",
          "shell.execute_reply": "2024-12-30T18:40:06.607233Z",
          "shell.execute_reply.started": "2024-12-30T18:40:06.102626Z"
        },
        "id": "awbFBy8v5rDM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(\"model-chatbot-medical-mew\")\n",
        "trainer.tokenizer.save_pretrained(\"model-chatbot-medical-mew\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_6XDhvKc37b"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DaDCbaMc37b"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(trainer.state.log_history)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['loss'], label='Training Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJGGWVExc37f"
      },
      "source": [
        "## Compare with MedAlpaca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ennfkv3vjRbE"
      },
      "source": [
        "We want to use [MedAlpaca](https://huggingface.co/medalpaca/medalpaca-7b) as benchmark to test the quality of our fine-tuning.\n",
        "\n",
        "***medalpaca-7b*** is a large language model specifically fine-tuned for medical domain tasks. It is based on LLaMA (Large Language Model Meta AI) and contains 7 billion parameters. The primary goal of this model is to improve question-answering and medical dialogue tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eAja8tHc37f"
      },
      "outputs": [],
      "source": [
        "# Function to download MedAlpaca model and tokenizer\n",
        "def setup_medAlpaca():\n",
        "    try:\n",
        "        # try not quantize model\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"medalpaca/medalpaca-7b\",\n",
        "            trust_remote_code=True,\n",
        "            device_map='auto',\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"medalpaca/medalpaca-7b\")\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(f\"Python version: {sys.version}\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OomPFh1yc37f"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "modelMED, tokenizerMED = setup_medAlpaca()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBj_UlApj4Fm"
      },
      "source": [
        "To evaluate our model response we use [BLEU](https://www.nltk.org/_modules/nltk/translate/bleu_score.html) score. BLEU's output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, with values closer to 1 representing more similar texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgPTQB_qc37f"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "# BLUE score between the candidate (generated answer) and the reference answer (taken from dataset)\n",
        "def calculate_bleu(reference, candidate):\n",
        "    try:\n",
        "      score = sentence_bleu(reference, candidate)\n",
        "      return score\n",
        "    except Exception as e:\n",
        "      print(f\"Error during BLUE computing: {e}\")\n",
        "      return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAS31dMvc37f"
      },
      "outputs": [],
      "source": [
        "# Take 10 instances from testset\n",
        "test = test_dataset[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbTmer27xNT5"
      },
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vyh3DvPc37f"
      },
      "outputs": [],
      "source": [
        "alpaca_bleu = []\n",
        "our_bleu = []\n",
        "\n",
        "# Preprompt used also from our model\n",
        "instruction = \"Answer this question truthfully: \"\n",
        "\n",
        "# medAlpaca\n",
        "print(\"MedAlpaca\")\n",
        "\n",
        "# A question is in the field input of an instance of the testset\n",
        "for i, question in enumerate(test['input']):\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "\n",
        "    input_text = instruction + question\n",
        "\n",
        "    # Tokenize\n",
        "    input_ids = tokenizerMED(input_text, return_tensors='pt').to(modelMED.device)[\"input_ids\"]\n",
        "\n",
        "    # Generate response\n",
        "    outputs = modelMED.generate(input_ids, max_new_tokens=128)\n",
        "    response = tokenizerMED.decode(outputs[0])\n",
        "\n",
        "    # Clean response by remove the question (NOT WORKING)\n",
        "    clean_response = re.sub(r'.*?\\? ', '', response, flags=re.DOTALL)\n",
        "\n",
        "    print(\"   Response MedAlpaca:\", clean_response)\n",
        "\n",
        "    # Comput BLEU score between generated answer and answer in testset\n",
        "    candidate = clean_response.split()\n",
        "    bleu_score = calculate_bleu(test['output'][i], candidate)\n",
        "\n",
        "    if bleu_score is not None:\n",
        "        print(f\"BLEU score for Llama answer {i}: {bleu_score}\")\n",
        "        # Append score to a list\n",
        "        alpaca_bleu.append(bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMpKFJWSyCrJ"
      },
      "outputs": [],
      "source": [
        "# Same code but for our model\n",
        "print(\"\\nOur Model\")\n",
        "\n",
        "for i, question in enumerate(test['input']):\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": instructions[0]},\n",
        "    {\"role\": \"user\", \"content\": question}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenize\n",
        "    model_inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(**model_inputs, max_new_tokens=128)\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    our = text.split(\"assistant\")[1]\n",
        "\n",
        "    print(\"Response Medical Meadow: \", our)\n",
        "\n",
        "    # Comput BLEU score between generated answer and answer in testset\n",
        "    candidate = our.split()\n",
        "    bleu_score = calculate_bleu(test['output'][i], candidate)\n",
        "\n",
        "    if bleu_score is not None:\n",
        "        print(f\"BLEU score for Medical Meadow answer {i}: {bleu_score}\")\n",
        "        # Append score to a list\n",
        "        our_bleu.append(bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72irnssYc37f"
      },
      "outputs": [],
      "source": [
        "# Table to view all scores\n",
        "table = list(zip([0,1,2,3,4,5,6,7,8,9], alpaca_bleu, our_bleu))\n",
        "print('BLEU score')\n",
        "print(tabulate(table, headers=['Question', 'MedAlpaca', 'MedicalMeadow'], tablefmt='grid'))\n",
        "\n",
        "# Average BLEU score for both models\n",
        "bleu_alpaca = np.mean(alpaca_bleu)\n",
        "bleu_med = np.mean(our_bleu)\n",
        "print(f\"\\n-> Average BLEU score for MedAlpaca model: {bleu_alpaca}\")\n",
        "print(f\"-> Average BLEU score for MedicalMeadow model: {bleu_med}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmC39wTYSsQ_"
      },
      "source": [
        "## Model Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwQNV8XLYoLX"
      },
      "source": [
        "#### Load pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-XhCQC1YhLP"
      },
      "outputs": [],
      "source": [
        "# Change with model folder\n",
        "trained_model = \"/kaggle/working/model-chatbot-medical-mew\"\n",
        "question = \"What does low Mobility suggest?\"\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    trained_model,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0},\n",
        "    torch_dtype=torch.float32,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-30T18:43:01.327614Z",
          "iopub.status.busy": "2024-12-30T18:43:01.327258Z",
          "iopub.status.idle": "2024-12-30T18:43:10.136652Z",
          "shell.execute_reply": "2024-12-30T18:43:10.135919Z",
          "shell.execute_reply.started": "2024-12-30T18:43:01.327584Z"
        },
        "id": "NL-rYUPJSsRD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "messages = [{\"role\": \"system\", \"content\": instructions[0]},\n",
        "    {\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "model_inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**model_inputs, max_new_tokens=128)\n",
        "\n",
        "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(text.split(\"assistant\")[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPKlNEM_sFu0"
      },
      "source": [
        "## Add voice interactivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QSzEjW2MZrc"
      },
      "source": [
        "### Record Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.852003Z",
          "iopub.status.idle": "2024-12-30T18:27:02.852305Z",
          "shell.execute_reply": "2024-12-30T18:27:02.852202Z"
        },
        "id": "7uLKcUMQMeaL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "References:\n",
        "https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/\n",
        "https://stackoverflow.com/a/18650249\n",
        "https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/\n",
        "https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/\n",
        "https://stackoverflow.com/a/49019356\n",
        "\"\"\"\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };\n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.852822Z",
          "iopub.status.idle": "2024-12-30T18:27:02.853094Z",
          "shell.execute_reply": "2024-12-30T18:27:02.852969Z"
        },
        "id": "oo1ink-uMiYo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Register audio\n",
        "audio, sr = get_audio()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.853655Z",
          "iopub.status.idle": "2024-12-30T18:27:02.853877Z",
          "shell.execute_reply": "2024-12-30T18:27:02.853788Z"
        },
        "id": "DaV9sVvZMlCL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save audio to file\n",
        "scipy.io.wavfile.write('./recording.wav', sr, audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1k0fMEMNtl2"
      },
      "source": [
        "## Speech to Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uRvjNElc37g"
      },
      "source": [
        "We tried normalize the audio but it will get a worse result. We resample at 16000 because for most speech-focused tasks, 16,000 Hz is optimal (8000 Hz is for telephones that have low bandwith and 44000 is for music)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zMag0Huc37g"
      },
      "outputs": [],
      "source": [
        "# Resample audio\n",
        "target_sample_rate = 16000\n",
        "audio, sr = librosa.load(\"recording.wav\", sr=None)  # Load with original sampling rate\n",
        "if sr != target_sample_rate:\n",
        "    audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sample_rate)\n",
        "\n",
        "# Save processed audio\n",
        "sf.write(\"processed_audio.wav\", audio, target_sample_rate)\n",
        "\n",
        "print(f\"Preprocessed audio saved as processed_audio.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.854542Z",
          "iopub.status.idle": "2024-12-30T18:27:02.854872Z",
          "shell.execute_reply": "2024-12-30T18:27:02.854712Z"
        },
        "id": "sPQmD992Lpjh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "whis = whisper.load_model(\"base\")\n",
        "\n",
        "# Load audio and pad/trim it to fit 30 seconds\n",
        "audio = whisper.load_audio(\"./processed_audio.wav\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# Plot the audio\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(audio, linewidth=0.4)\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlabel('Samples')\n",
        "plt.show()\n",
        "\n",
        "# Move log-Mel spectrogram to the same device as the model\n",
        "mel = whisper.log_mel_spectrogram(audio).to(whis.device)\n",
        "\n",
        "# Visualize spectrogram\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "plt.pcolormesh(mel.cpu().numpy())\n",
        "plt.colorbar(label='Power [dB]')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.xlabel('Time [10ms]')\n",
        "plt.show()\n",
        "\n",
        "# Use the mel spectrogram to detect the language\n",
        "_, probs = whis.detect_language(mel)\n",
        "lang = max(probs, key=probs.get)\n",
        "\n",
        "# Print result\n",
        "print(f\"Detected language: {lang}, confidence: {probs[lang]:.3f}\")\n",
        "\n",
        "# Decode the audio\n",
        "options = whisper.DecodingOptions(fp16 = False)\n",
        "result = whisper.decode(whis, mel, options)\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.855837Z",
          "iopub.status.idle": "2024-12-30T18:27:02.856217Z",
          "shell.execute_reply": "2024-12-30T18:27:02.856058Z"
        },
        "id": "La6g7PIIN3yo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Dump result text into model\n",
        "inputs = tokenizer(result.text, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "output = model.generate(**inputs)\n",
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZLo43qaNyqn"
      },
      "source": [
        "## Text to Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.857137Z",
          "iopub.status.idle": "2024-12-30T18:27:02.857562Z",
          "shell.execute_reply": "2024-12-30T18:27:02.857350Z"
        },
        "id": "DVEewwceOF-Y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load Tacotron model\n",
        "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\n",
        "tacotron2 = tacotron2.to('cuda')\n",
        "tacotron2.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.858937Z",
          "iopub.status.idle": "2024-12-30T18:27:02.859342Z",
          "shell.execute_reply": "2024-12-30T18:27:02.859184Z"
        },
        "id": "tcKqPWGXOJz9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load Waveglow\n",
        "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp16')\n",
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow = waveglow.to('cuda')\n",
        "waveglow.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.860028Z",
          "iopub.status.idle": "2024-12-30T18:27:02.860393Z",
          "shell.execute_reply": "2024-12-30T18:27:02.860239Z"
        },
        "id": "DlHbL8_YOMON",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Add model response\n",
        "print(tokenizer.decode(output[0]))\n",
        "text = tokenizer.decode(output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.861323Z",
          "iopub.status.idle": "2024-12-30T18:27:02.861724Z",
          "shell.execute_reply": "2024-12-30T18:27:02.861556Z"
        },
        "id": "SYGrvo4YOOBX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load tts utils\n",
        "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')\n",
        "sequences, lengths = utils.prepare_input_sequence([text])\n",
        "sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.862526Z",
          "iopub.status.idle": "2024-12-30T18:27:02.862883Z",
          "shell.execute_reply": "2024-12-30T18:27:02.862729Z"
        },
        "id": "oOyIqRq9OQJP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    mel, _, _ = tacotron2.infer(sequences, lengths)\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "plt.pcolormesh(mel[0].cpu().numpy())\n",
        "plt.colorbar(label='Power [dB]')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.xlabel('Time [10ms]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.863553Z",
          "iopub.status.idle": "2024-12-30T18:27:02.863999Z",
          "shell.execute_reply": "2024-12-30T18:27:02.863754Z"
        },
        "id": "t-dE1l-dOVwn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    audio = waveglow.infer(mel)\n",
        "audio_numpy = audio[0].data.cpu().numpy()\n",
        "rate = 22050\n",
        "\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(audio_numpy, linewidth=0.4)\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlabel('Samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-30T18:27:02.864777Z",
          "iopub.status.idle": "2024-12-30T18:27:02.865156Z",
          "shell.execute_reply": "2024-12-30T18:27:02.864977Z"
        },
        "id": "PNwE-1SuObFF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Play response audio\n",
        "Audio(audio_numpy, rate=rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgu3JQjGc37i"
      },
      "source": [
        "## Potential Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C_3YgMPtprd"
      },
      "source": [
        "### Improved Audio\n",
        "\n",
        "We use a bigger model to recognize the speech, tha base model is whisper but now we use the larger version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSQcFys7c37i"
      },
      "outputs": [],
      "source": [
        "# Setting the device\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# Loading the model\n",
        "model_id = \"openai/whisper-large-v3\"\n",
        "\n",
        "model_stp = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model_stp.to(device)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model_stp,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Inference the model\n",
        "result = pipe(\"recording.wav\")\n",
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H9L3uHQc37i"
      },
      "outputs": [],
      "source": [
        "# Inference the model with our question\n",
        "question = result[\"text\"]\n",
        "print(question)\n",
        "\n",
        "# Loading the fine-tuned model and the tokenizer for inference\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"model\",\n",
        "        max_seq_length = 2048,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "# Enable faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Set the right template for the question\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"Answer this question truthfully:{question}\"},\n",
        "]\n",
        "\n",
        "# Standard generation\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "print(\"\\n Generation:\")\n",
        "streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "\n",
        "response = model.generate(\n",
        "    input_ids = inputs,\n",
        "    streamer = streamer,\n",
        "    max_new_tokens = 64,\n",
        "    use_cache = True,\n",
        "    temperature = 0.7,\n",
        "    min_p = 0.1\n",
        ")\n",
        "\n",
        "# Get the response\n",
        "resp = tokenizer.decode(response[0])\n",
        "lst_resp = resp.split(\"\\n\")\n",
        "print(lst_resp[-1])\n",
        "response = str(lst_resp[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S56zwOs2c37i"
      },
      "outputs": [],
      "source": [
        "# Load text to speech model\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\n",
        "tts.to(device)\n",
        "\n",
        "# Generate speech by cloning a voice\n",
        "tts.tts_to_file(text=response,\n",
        "                file_path=\"output_tts.wav\",\n",
        "                speaker_wav=\"obama_audio.mp3\",\n",
        "                language=\"en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFCg_Vsdc37i"
      },
      "outputs": [],
      "source": [
        "# Audio with generated response\n",
        "Audio(\"output_tts.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgl6chy1cPsY"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtOVoYxocULS"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR87HRbfce2i"
      },
      "source": [
        "LoRA adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-0tWbxschYi"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MovmBY3c3Rm"
      },
      "source": [
        "Format Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w8bQdnhc9G3"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\", split=\"train\")\n",
        "\n",
        "def convert_to_conversations(examples):\n",
        "    \"\"\"Convert the dataset format to conversations\"\"\"\n",
        "    conversations = []\n",
        "    for instruction, input_text, output in zip(\n",
        "        examples['instruction'],\n",
        "        examples['input'],\n",
        "        examples['output']\n",
        "    ):\n",
        "        # Combine instruction and input\n",
        "        user_content = f\"{instruction}: {input_text}\"\n",
        "        conv = [\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "            {\"role\": \"assistant\", \"content\": output}\n",
        "        ]\n",
        "        conversations.append(conv)\n",
        "    return {\"conversations\": conversations}\n",
        "\n",
        "# Convert to conversation format\n",
        "formatted_dataset = dataset.map(\n",
        "    convert_to_conversations,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "# Set up the tokenizer with Llama-3.1 chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"Apply the chat template to the conversations\"\"\"\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            convo,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        ) for convo in convos\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply the formatting\n",
        "formatted_dataset = formatted_dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9acv847bdClb"
      },
      "outputs": [],
      "source": [
        "# Print an example to verify the format\n",
        "print(\"Example of formatted conversation:\")\n",
        "print(formatted_dataset[0]['text'])\n",
        "\n",
        "# The dataset is now ready for training\n",
        "# You can access it as formatted_dataset['text']\n",
        "train_dataset, val_dataset, test_dataset = random_split(formatted_dataset, [0.8, 0.1, 0.1])\n",
        "# If you need to split it into train/validation sets:\n",
        "train_val = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_data = train_val['train']\n",
        "val_data = train_val['test']\n",
        "\n",
        "print(\"\\nDataset sizes:\")\n",
        "print(f\"Train: {len(train_data)}\")\n",
        "print(f\"Validation: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kca4Wt6dGMb"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OCmnk9fdNuB"
      },
      "outputs": [],
      "source": [
        "# Preparazione della collazione per il dataset\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
        "\n",
        "# Trainer con valutazione e senza report\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_data,  # Dataset di training\n",
        "    # eval_dataset=val_data,  # Dataset di validazione\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    data_collator=data_collator,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4,  # Batch di training\n",
        "        per_device_eval_batch_size=8,  # Batch di valutazione\n",
        "        gradient_accumulation_steps=8,\n",
        "        warmup_steps=10,\n",
        "        max_steps=30,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=True,  # Precisione misto\n",
        "        logging_steps=2,  # Loggare ogni 10 passi\n",
        "        # eval_strategy=\"steps\",  # Attiva la valutazione\n",
        "        # eval_steps=2,  # Frequenza della valutazione\n",
        "        save_steps=50,  # Frequenza del salvataggio dei checkpoint\n",
        "        save_total_limit=2,  # Limita il numero di checkpoint salvati\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=42,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # Disabilita il report\n",
        "        # load_best_model_at_end=True,  # Carica il miglior modello alla fine\n",
        "        metric_for_best_model=\"eval_loss\",  # Usa la perdita come metrica\n",
        "        greater_is_better=False,  # Perdita minore è meglio\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgiZ1Fi7dT5H"
      },
      "outputs": [],
      "source": [
        "#We also use Unsloth's train_on_completions method to only train on the assistant outputs and ignore the loss on the user's inputs.\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEaUPiqzdVrS"
      },
      "outputs": [],
      "source": [
        "# We verify masking is actually done:\n",
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5kkR3xvdhRw"
      },
      "outputs": [],
      "source": [
        "# Get the token ID for a space\n",
        "space_token_id = tokenizer(\" \", add_special_tokens=False).input_ids[0]\n",
        "\n",
        "# Process the labels, replacing -100 with the space token ID\n",
        "labels = trainer.train_dataset[5][\"labels\"]\n",
        "processed_labels = [space_token_id if token == -100 else token for token in labels]\n",
        "\n",
        "# Decode the processed labels into text\n",
        "decoded_text = tokenizer.decode(processed_labels)\n",
        "\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_1w3r-ud7pc"
      },
      "outputs": [],
      "source": [
        "# See the stats , it was curios to see what is saved during the training\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fB3HtLtd0-W"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3HYffPXc37h"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(trainer.state.log_history)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['loss'], label='Training Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmAOlPjQeJRX"
      },
      "outputs": [],
      "source": [
        "#Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oenKzibBlSnr"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save_pretrained(\"finetuned_unsloth_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"finetuned_unsloth_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mb8Yzs9eL15"
      },
      "source": [
        "Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_20v1bbjeLZw"
      },
      "outputs": [],
      "source": [
        "# Setup tokenizer with Llama-3.1 template\n",
        "\n",
        "\n",
        "# Loading the fine-tuned model and the tokenizer for inference\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"model\",\n",
        "        max_seq_length = 2048,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "\n",
        "# Enable faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Example medical question from our dataset\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Answer this question truthfully: What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\"},\n",
        "]\n",
        "\n",
        "# Standard generation\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "print(\"\\n Generation:\")\n",
        "streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "\n",
        "_ = model.generate(\n",
        "    input_ids = inputs,\n",
        "    streamer = streamer,\n",
        "    max_new_tokens = 64,\n",
        "    use_cache = True,\n",
        "    temperature = 0.7,\n",
        "    min_p = 0.1\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XKL46PcIc21t",
        "PJljpW-Jc37a",
        "f4wIndWS2lyy",
        "-BMh1vbuc5qp",
        "a_6XDhvKc37b",
        "vR87HRbfce2i",
        "4MovmBY3c3Rm",
        "5kca4Wt6dGMb",
        "uJGGWVExc37f",
        "3QSzEjW2MZrc"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30823,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}