{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sAndreotti/MedicalMeadow/blob/llama3.2-test/ATML_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets accelerate peft bitsandbytes transformers trl==0.12.0 plotly huggingface_hub\n",
        "!pip install --upgrade smart_open\n",
        "!pip install --upgrade gensim\n",
        "!pip install ffmpeg-python"
      ],
      "metadata": {
        "id": "SAho3HGib9-U",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from trl import SFTTrainer\n",
        "import re\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import plotly.express as px\n",
        "import random\n",
        "from sklearn.manifold import TSNE\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import Dataset as HFDataset"
      ],
      "metadata": {
        "id": "q_JimYqjjY4S",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Investigate Dataset"
      ],
      "metadata": {
        "id": "XKL46PcIc21t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
        "ds = ds['train']\n",
        "ds"
      ],
      "metadata": {
        "id": "YltBm7i4b0IM",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds.features, \"\\n\")\n",
        "print(\"Instruction:\")\n",
        "print(f\"length: {len(ds['instruction'])}\")\n",
        "print(f\"example: {ds['instruction'][0]}\")\n",
        "print()\n",
        "\n",
        "print(f\"Input:\")\n",
        "print(f\"length: {len(ds['input'])}\")\n",
        "print(f\"example: {ds['input'][0]}\")\n",
        "print()\n",
        "\n",
        "print(f\"Output:\")\n",
        "print(f\"length: {len(ds['output'])}\")\n",
        "print(f\"example: {ds['output'][0]}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "n89RynbpcHQB",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some plots about the dataset"
      ],
      "metadata": {
        "id": "a9SXu1UFoE4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the 'instruction' field\n",
        "instructions = ds['instruction']\n",
        "\n",
        "# Count the frequency of each unique instruction\n",
        "instruction_counts = {instruction: instructions.count(instruction) for instruction in set(instructions)}\n",
        "\n",
        "# Sort the instructions by frequency\n",
        "sorted_instructions = sorted(instruction_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Separate the instructions and their counts for plotting\n",
        "sorted_instruction_names = [item[0] for item in sorted_instructions]\n",
        "sorted_instruction_counts = [item[1] for item in sorted_instructions]\n",
        "\n",
        "# Plotting the frequency of instructions\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bars = plt.barh(sorted_instruction_names, sorted_instruction_counts, color='skyblue', edgecolor='black', linewidth=1.2)\n",
        "plt.title('Instruction Frequency Distribution')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Instruction')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4TAMV5DdnRg7",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrases = ds['input']\n",
        "output_phrases = ds['output']\n",
        "\n",
        "# Calculate the length of each phrase\n",
        "input_lengths = [len(phrase) for phrase in input_phrases]\n",
        "output_lengths = [len(phrase) for phrase in output_phrases]\n",
        "\n",
        "# Define the bins for the length ranges\n",
        "max_input = max(input_lengths)\n",
        "max_output = max(output_lengths)\n",
        "\n",
        "input_bins = [i * max_input / 10 for i in range(1, 11)]\n",
        "output_bins = [i * max_output / 10 for i in range(1, 11)]\n",
        "bin_labels_input = [f'{int(input_bins[i-1])}-{int(input_bins[i])}' for i in range(1, 10)]\n",
        "bin_labels_output = [f'{int(output_bins[i-1])}-{int(output_bins[i])}' for i in range(1, 10)]\n",
        "\n",
        "# Bin the lengths into the categories\n",
        "input_binned = np.digitize(input_lengths, input_bins)  # Categorize based on input lengths\n",
        "output_binned = np.digitize(output_lengths, output_bins)  # Categorize based on output lengths\n",
        "\n",
        "# Count how many phrases fall into each bin\n",
        "input_bin_counts = [sum(input_binned == i) for i in range(1, len(input_bins))]\n",
        "output_bin_counts = [sum(output_binned == i) for i in range(1, len(output_bins))]\n",
        "\n",
        "# Plotting the bar charts\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Plotting the input phrase lengths\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(bin_labels_input, input_bin_counts, color='skyblue', edgecolor='black')\n",
        "plt.title('Input Phrases Length Distribution')\n",
        "plt.xlabel('Length Range')\n",
        "plt.ylabel('Number of Phrases')\n",
        "\n",
        "# Plotting the output phrase lengths\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(bin_labels_output, output_bin_counts, color='skyblue', edgecolor='black')\n",
        "plt.title('Output Phrases Length Distribution')\n",
        "plt.xlabel('Length Range')\n",
        "plt.ylabel('Number of Phrases')\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d9lEMELsiqIx",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "7zqoFYTT2lyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = [re.sub('\\W', ' ', sentence).lower().split() for sentence in input_phrases]\n",
        "# remove sentences that are only 1 word long\n",
        "tokenized_sentences = [sentence for sentence in tokenized_sentences if len(sentence) > 1]\n",
        "\n",
        "for sentence in tokenized_sentences[:5]:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wRH-mf5T2lyy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# merge in & out togheter\n",
        "# merged_list = [f\"{a} {b}\" for a, b in zip(input_phrases, output_phrases)]\n",
        "\n",
        "# remove newline characters\n",
        "# docs = [re.sub('\\n', ' ', doc) for doc in merged_list]\n",
        "# split sentences\n",
        "#sentences = [re.split('[?!.]\\s', doc) for doc in docs]\n",
        "#sentences[:3]"
      ],
      "metadata": {
        "trusted": true,
        "id": "yZ8ZCHgj2lyy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from pandas.core.common import flatten\n",
        "\n",
        "# sentences = list(flatten(sentences))\n",
        "# sentences[:20]"
      ],
      "metadata": {
        "trusted": true,
        "id": "yedEUf_32lyy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "f4wIndWS2lyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(tokenized_sentences, vector_size=30, min_count=5, window=10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "e05BkgN-2lyz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample = random.sample(list(model.wv.key_to_index), 500)\n",
        "word_vectors = model.wv[sample]"
      ],
      "metadata": {
        "trusted": true,
        "id": "blQAYdGr2lyz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3D plot with words"
      ],
      "metadata": {
        "id": "CskZCC7a2lyz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "tMGw4YRE2lyz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=3, n_iter=2000)\n",
        "tsne_embedding = tsne.fit_transform(word_vectors)\n",
        "\n",
        "x, y, z = np.transpose(tsne_embedding)\n",
        "\n",
        "fig = px.scatter_3d(x=x[:200],y=y[:200],z=z[:200],text=sample[:200])\n",
        "fig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\n",
        "fig.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_0EGqbEA2lyz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "first_question = ['man', 'woman']\n",
        "#question = ['rem', 'sleep', 'hallucinations', 'paralysis']\n",
        "\n",
        "word_vectors = model.wv[first_question+sample]\n",
        "\n",
        "tsne = TSNE(n_components=3)\n",
        "tsne_embedding = tsne.fit_transform(word_vectors)\n",
        "\n",
        "x, y, z = np.transpose(tsne_embedding)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ruxbKZBt2lyz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "r = (-200,200)\n",
        "fig = px.scatter_3d(x=x, y=y, z=z, range_x=r, range_y=r, range_z=r, text=first_question + [None] * 500)\n",
        "fig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\n",
        "fig.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Cem4IiRb2lyz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('menopause')"
      ],
      "metadata": {
        "trusted": true,
        "id": "E795I6ej2lyz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "vec = model.wv.get_vector('headache') + (model.wv.get_vector('fever') - model.wv.get_vector('drug'))\n",
        "model.wv.similar_by_vector(vec)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8RkQLdpv2lyz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate models"
      ],
      "metadata": {
        "id": "-BMh1vbuc5qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create dataset"
      ],
      "metadata": {
        "id": "VFt5KLR-8w3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedDataset(Dataset):\n",
        "  def __init__(self, instruction, input, output):\n",
        "    self.instruction = instruction\n",
        "    self.input = input\n",
        "    self.output = output\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.instruction)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sentence = \"<s>[INST] \"+self.instruction[idx]+\". \"+self.input[idx]+\" [/INST] \"+self.output[idx]+\" </s>\"\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "ND0M0R633VbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "garnachoDataset = MedDataset(ds['instruction'], ds['input'], ds['output'])\n",
        "for i in range(5):\n",
        "  print(garnachoDataset[i])"
      ],
      "metadata": {
        "id": "C6gCSyFM7IsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(garnachoDataset)"
      ],
      "metadata": {
        "id": "xLFFgWOWDIg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = random_split(garnachoDataset, [0.8, 0.1, 0.1])\n",
        "print(len(train_dataset))\n",
        "print(len(val_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "id": "RRneCjBj9Lag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=\"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\")\n",
        "\n",
        "base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "Ak2dRuLhdExx",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_representation=\"nested\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0},\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "gR4BtF8AHD0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "fLvCmsgJNbqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_params = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "ew98HuwaKW1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\",\n",
        "    gradient_checkpointing=True\n",
        ")"
      ],
      "metadata": {
        "id": "n8dBHX-j-M1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_hf_dataset(med_dataset):\n",
        "    # Create lists to store all formatted text\n",
        "    formatted_texts = []\n",
        "\n",
        "    # Iterate through all items in the original dataset\n",
        "    for idx in range(len(med_dataset.instruction)):\n",
        "        # Get the formatted text directly using the dataset's __getitem__\n",
        "        formatted_text = med_dataset[idx]\n",
        "        formatted_texts.append(formatted_text)\n",
        "\n",
        "    # Create a dictionary with the required format\n",
        "    dataset_dict = {\n",
        "        'text': formatted_texts\n",
        "    }\n",
        "\n",
        "    # Convert to HuggingFace Dataset\n",
        "    hf_dataset = HFDataset.from_dict(dataset_dict)\n",
        "\n",
        "    return hf_dataset\n",
        "\n",
        "hf_dataset = convert_to_hf_dataset(garnachoDataset)"
      ],
      "metadata": {
        "id": "UXLnghrqDWIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=hf_dataset,\n",
        "    peft_config=peft_params,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_params,\n",
        "    packing=False,\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "O_ZUEMS02lyz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "1wEjsaFf9ggS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.save_pretrained(\"model-chatbot-medical-mew\")\n",
        "trainer.tokenizer.save_pretrained(\"model-chatbot-medical-mew\")"
      ],
      "metadata": {
        "id": "OBklnGpq6Sgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add voice interactivity"
      ],
      "metadata": {
        "id": "kPKlNEM_sFu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Record Audio"
      ],
      "metadata": {
        "id": "3QSzEjW2MZrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "References:\n",
        "https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/\n",
        "https://stackoverflow.com/a/18650249\n",
        "https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/\n",
        "https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/\n",
        "https://stackoverflow.com/a/49019356\n",
        "\"\"\"\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };\n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr"
      ],
      "metadata": {
        "id": "7uLKcUMQMeaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio, sr = get_audio()"
      ],
      "metadata": {
        "id": "oo1ink-uMiYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "scipy.io.wavfile.write('./tmp/recording.wav', sr, audio)"
      ],
      "metadata": {
        "id": "DaV9sVvZMlCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech to Text"
      ],
      "metadata": {
        "id": "_1k0fMEMNtl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper"
      ],
      "metadata": {
        "trusted": true,
        "id": "vsJm-JR3sFu5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# load audio and pad/trim it to fit 30 seconds\n",
        "audio = whisper.load_audio(\"./tmp/recording.wav\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# load audio and pad/trim it to fit 30 seconds\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(audio, linewidth=0.4)\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlabel('Samples')\n",
        "plt.show()\n",
        "\n",
        "# make log-Mel spectrogram and move to the same device as the model\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "# Visualize spectrogram\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "plt.pcolormesh(mel.cpu().numpy())\n",
        "plt.colorbar(label='Power [dB]')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.xlabel('Time [10ms]')\n",
        "plt.show()\n",
        "\n",
        "# Use the mel spectrogram to detect the language\n",
        "_, probs = model.detect_language(mel)\n",
        "lang = max(probs, key=probs.get)\n",
        "\n",
        "# Print result\n",
        "print(f\"Detected language: {lang}, confidence: {probs[lang]}\")\n",
        "\n",
        "# decode the audio\n",
        "options = whisper.DecodingOptions(fp16 = False)\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(result.text)"
      ],
      "metadata": {
        "id": "sPQmD992Lpjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Dump result text into model"
      ],
      "metadata": {
        "id": "La6g7PIIN3yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text to Speech"
      ],
      "metadata": {
        "id": "LZLo43qaNyqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy scipy librosa unidecode inflect\n",
        "#!apt-get update\n",
        "#!apt-get install -y libsndfile1"
      ],
      "metadata": {
        "id": "ERbGYil6NyLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\n",
        "tacotron2 = tacotron2.to('cuda')\n",
        "tacotron2.eval()"
      ],
      "metadata": {
        "id": "DVEewwceOF-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp16')\n",
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow = waveglow.to('cuda')\n",
        "waveglow.eval()"
      ],
      "metadata": {
        "id": "tcKqPWGXOJz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Add model response\n",
        "text = \"And you know what they call a Quarter Pounder with Cheese in Paris?\""
      ],
      "metadata": {
        "id": "DlHbL8_YOMON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')\n",
        "sequences, lengths = utils.prepare_input_sequence([text])\n",
        "sequences"
      ],
      "metadata": {
        "id": "SYGrvo4YOOBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    mel, _, _ = tacotron2.infer(sequences, lengths)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "plt.pcolormesh(mel[0].cpu().numpy())\n",
        "plt.colorbar(label='Power [dB]')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.xlabel('Time [10ms]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oOyIqRq9OQJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    audio = waveglow.infer(mel)\n",
        "audio_numpy = audio[0].data.cpu().numpy()\n",
        "rate = 22050\n",
        "\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(audio_numpy, linewidth=0.4)\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlabel('Samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t-dE1l-dOVwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio(audio_numpy, rate=rate)"
      ],
      "metadata": {
        "id": "PNwE-1SuObFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Potential extensions"
      ],
      "metadata": {
        "id": "s22bSn7_sFu6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1mPCVNJdGSr",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}