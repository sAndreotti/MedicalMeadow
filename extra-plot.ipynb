{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"gpuType":"T4","provenance":[],"include_colab_link":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/sAndreotti/MedicalMeadow/blob/main/ATML_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"Import Libraries","metadata":{"id":"29nxqgohSsQ3"}},{"cell_type":"code","source":"!pip install datasets accelerate peft bitsandbytes transformers trl==0.12.0 plotly huggingface_hub\n!pip install --upgrade smart_open\n!pip install --upgrade gensim\n!pip install ffmpeg-python\n!pip install -U openai-whisper\n!pip install scipy librosa unidecode inflect","metadata":{"id":"SAho3HGib9-U","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\nfrom trl import SFTTrainer\nimport re\nfrom gensim.models.word2vec import Word2Vec\nimport plotly.express as px\nimport random\nfrom sklearn.manifold import TSNE\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import random_split\nfrom peft import prepare_model_for_kbit_training, LoraConfig\nfrom huggingface_hub import login\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments\n)\nfrom datasets import Dataset as HFDataset\nfrom peft import AutoPeftModelForCausalLM\nimport whisper\nfrom IPython.display import Audio","metadata":{"id":"q_JimYqjjY4S","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Import Libraries for audio part","metadata":{"id":"E2DDMWKGSsQ5"}},{"cell_type":"code","source":"from IPython.display import HTML, Audio\nfrom google.colab.output import eval_js\nfrom base64 import b64decode\nfrom scipy.io.wavfile import read as wav_read\nimport io\nimport ffmpeg\nimport scipy","metadata":{"id":"lkjSoQT4SsQ6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Investigate Dataset","metadata":{"id":"XKL46PcIc21t"}},{"cell_type":"code","source":"ds = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\nds = ds['train']\nds","metadata":{"id":"YltBm7i4b0IM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ds.features, \"\\n\")\nprint(\"Instruction:\")\nprint(f\"length: {len(ds['instruction'])}\")\nprint(f\"example: {ds['instruction'][0]} \\n\")\n\nprint(f\"Input:\")\nprint(f\"length: {len(ds['input'])}\")\nprint(f\"example: {ds['input'][0]} \\n\")\n\nprint(f\"Output:\")\nprint(f\"length: {len(ds['output'])}\")\nprint(f\"example: {ds['output'][0]} \\n\")","metadata":{"id":"n89RynbpcHQB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Some plots about the dataset","metadata":{"id":"a9SXu1UFoE4x"}},{"cell_type":"code","source":"instructions = ds['instruction']\ninput_phrases = ds['input']\noutput_phrases = ds['output']","metadata":{"trusted":true,"id":"nLEtHKtISsQ7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the frequency of each unique instruction\ninstruction_counts = {instruction: instructions.count(instruction) for instruction in set(instructions)}\n\n# Sort the instructions by frequency\nsorted_instructions = sorted(instruction_counts.items(), key=lambda x: x[1], reverse=True)\n\n# Separate the instructions and their counts for plotting\nsorted_instruction_names = [item[0] for item in sorted_instructions]\nsorted_instruction_counts = [item[1] for item in sorted_instructions]\n\n# Plotting the frequency of instructions\nplt.figure(figsize=(10, 5))\n\nbars = plt.barh(sorted_instruction_names, sorted_instruction_counts, color='skyblue', edgecolor='black', linewidth=1.2)\nplt.title('Instruction Frequency Distribution')\nplt.xlabel('Frequency')\nplt.ylabel('Instruction')\n\n# Show the plot\nplt.tight_layout()\nplt.show()","metadata":{"id":"4TAMV5DdnRg7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the length of each phrase\ninput_lengths = [len(phrase) for phrase in input_phrases]\noutput_lengths = [len(phrase) for phrase in output_phrases]\n\n# Define the bins for the length ranges\nmax_input = max(input_lengths)\nmax_output = max(output_lengths)\n\ninput_bins = [i * max_input / 10 for i in range(1, 11)]\noutput_bins = [i * max_output / 10 for i in range(1, 11)]\nbin_labels_input = [f'{int(input_bins[i-1])}-{int(input_bins[i])}' for i in range(1, 10)]\nbin_labels_output = [f'{int(output_bins[i-1])}-{int(output_bins[i])}' for i in range(1, 10)]\n\n# Bin the lengths into the categories\ninput_binned = np.digitize(input_lengths, input_bins)  # Categorize based on input lengths\noutput_binned = np.digitize(output_lengths, output_bins)  # Categorize based on output lengths\n\n# Count how many phrases fall into each bin\ninput_bin_counts = [sum(input_binned == i) for i in range(1, len(input_bins))]\noutput_bin_counts = [sum(output_binned == i) for i in range(1, len(output_bins))]\n\n# Plotting the bar charts\nplt.figure(figsize=(20, 10))\n\n# Plotting the input phrase lengths\nplt.subplot(1, 2, 1)\nplt.bar(bin_labels_input, input_bin_counts, color='skyblue', edgecolor='black')\nplt.title('Input Phrases Length Distribution')\nplt.xlabel('Length Range')\nplt.ylabel('Number of Phrases')\n\n# Plotting the output phrase lengths\nplt.subplot(1, 2, 2)\nplt.bar(bin_labels_output, output_bin_counts, color='skyblue', edgecolor='black')\nplt.title('Output Phrases Length Distribution')\nplt.xlabel('Length Range')\nplt.ylabel('Number of Phrases')\n\n# Show the plots\nplt.tight_layout()\nplt.show()","metadata":{"id":"d9lEMELsiqIx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Calculate the length of each phrase\ninput_lengths = [len(phrase) for phrase in input_phrases]\noutput_lengths = [len(phrase) for phrase in output_phrases]\n\n# Define the bins for the length ranges\nmax_input = max(input_lengths)\nmax_output = max(output_lengths)\n\ninput_bins = [i * max_input / 10 for i in range(1, 11)]\noutput_bins = [i * max_output / 10 for i in range(1, 11)]\nbin_labels_input = [f'{int(input_bins[i-1])}-{int(input_bins[i])}' for i in range(1, 10)]\nbin_labels_output = [f'{int(output_bins[i-1])}-{int(output_bins[i])}' for i in range(1, 10)]\n\n# Bin the lengths into the categories\ninput_binned = np.digitize(input_lengths, input_bins)  # Categorize based on input lengths\noutput_binned = np.digitize(output_lengths, output_bins)  # Categorize based on output lengths\n\n# Count how many phrases fall into each bin\ninput_bin_counts = [sum(input_binned == i) for i in range(1, len(input_bins))]\noutput_bin_counts = [sum(output_binned == i) for i in range(1, len(output_bins))]\n\n# Plotting the bar charts with improved styling\nplt.figure(figsize=(20, 10))\n\n# Plotting the input phrase lengths\nplt.subplot(1, 2, 1)\nbars_input = plt.bar(bin_labels_input, input_bin_counts, color='#FF6347', edgecolor='black', linewidth=1.2)\nplt.title('Input Phrases Length Distribution', fontsize=16, weight='bold', color='#2F4F4F')\nplt.xlabel('Length Range', fontsize=14, color='#2F4F4F')\nplt.ylabel('Number of Phrases', fontsize=14, color='#2F4F4F')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add data labels on bars for input\nfor bar in bars_input:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, int(yval), ha='center', fontsize=12, color='black')\n\n# Plotting the output phrase lengths\nplt.subplot(1, 2, 2)\nbars_output = plt.bar(bin_labels_output, output_bin_counts, color='#4682B4', edgecolor='black', linewidth=1.2)\nplt.title('Output Phrases Length Distribution', fontsize=16, weight='bold', color='#2F4F4F')\nplt.xlabel('Length Range', fontsize=14, color='#2F4F4F')\nplt.ylabel('Number of Phrases', fontsize=14, color='#2F4F4F')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add data labels on bars for output\nfor bar in bars_output:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, int(yval), ha='center', fontsize=12, color='black')\n\n# Show the plots\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Extra Plot","metadata":{}},{"cell_type":"code","source":"# Word cloud\nfrom wordcloud import WordCloud\n\ninput = ' '.join(input_phrases)\noutput = ' '.join(output_phrases)\n\nwordcloud1 = WordCloud(width=800, height=400, background_color='white').generate(input)\nwordcloud2 = WordCloud(width=800, height=400, background_color='white').generate(output)\n\n# Plotting the bar charts\nplt.figure(figsize=(20, 10))\n\n# Plotting the input phrase lengths\nplt.subplot(1, 2, 1)\nplt.imshow(wordcloud1, interpolation='bilinear')\nplt.axis('off')\nplt.title('Question Word Cloud')\n\n# Plotting the output phrase lengths\nplt.subplot(1, 2, 2)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis('off')\nplt.title('Answer Word Cloud')\n\n# Show the plots\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Converti input_phrases in una Series\ninput_series = pd.Series(input_phrases)\n\n# Applica il preprocessing\ninput_series = input_series.apply(preprocess)\n\n# Conta le parole\nword_counts = Counter([word for tokens in input_series for word in tokens])\nmost_common = word_counts.most_common(20)\n\n# Prepara i dati per il grafico\nwords1, counts1 = zip(*most_common)\n\n# Converti output_phrases in una Series\noutput_series = pd.Series(output_phrases)\n\n# Applica il preprocessing\noutput_series = output_series.apply(preprocess)\n\n# Conta le parole\nword_counts = Counter([word for tokens in output_series for word in tokens])\nmost_common = word_counts.most_common(20)\n\n# Prepara i dati per il grafico\nwords2, counts2 = zip(*most_common)\n\n# Plotting the bar charts side by side\nplt.figure(figsize=(20, 10))\n\n# First subplot (Input Phrases)\nplt.subplot(1, 2, 1)\nsns.barplot(x=list(counts1), y=list(words1), palette='viridis')\nplt.title('Top 20 Most Frequent Words in Input Phrases')\nplt.xlabel('Frequency')\nplt.ylabel('Words')\n\n# Second subplot (Output Phrases)\nplt.subplot(1, 2, 2)\nsns.barplot(x=list(counts2), y=list(words2), palette='viridis')\nplt.title('Top 20 Most Frequent Words in Output Phrases')\nplt.xlabel('Frequency')\nplt.ylabel('Words')\n\n# Adjust layout and show the plots\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}