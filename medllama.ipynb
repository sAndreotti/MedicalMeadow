{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"include_colab_link":true},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":218074,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":185968,"modelId":208088}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/sAndreotti/MedicalMeadow/blob/main/ATML_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"Import Libraries","metadata":{"id":"29nxqgohSsQ3"}},{"cell_type":"code","source":"!pip install datasets accelerate peft bitsandbytes transformers trl==0.12.0 plotly huggingface_hub\n!pip install --upgrade smart_open\n!pip install --upgrade gensim\n!pip install ffmpeg-python\n!pip install -U openai-whisper\n!pip install scipy librosa unidecode inflect","metadata":{"id":"SAho3HGib9-U","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\nfrom trl import SFTTrainer\nimport re\nfrom gensim.models.word2vec import Word2Vec\nimport plotly.express as px\nimport random\nfrom sklearn.manifold import TSNE\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import random_split\nfrom peft import prepare_model_for_kbit_training, LoraConfig\nfrom huggingface_hub import login\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments\n)\nfrom datasets import Dataset as HFDataset\nfrom peft import AutoPeftModelForCausalLM\nimport whisper\nfrom IPython.display import Audio","metadata":{"id":"q_JimYqjjY4S","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Import Libraries for audio part","metadata":{"id":"E2DDMWKGSsQ5"}},{"cell_type":"code","source":"from IPython.display import HTML, Audio\nfrom google.colab.output import eval_js\nfrom base64 import b64decode\nfrom scipy.io.wavfile import read as wav_read\nimport io\nimport ffmpeg\nimport scipy","metadata":{"id":"lkjSoQT4SsQ6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Investigate Dataset","metadata":{"id":"XKL46PcIc21t"}},{"cell_type":"code","source":"ds = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\nds = ds['train']\nds","metadata":{"id":"YltBm7i4b0IM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ds.features, \"\\n\")\nprint(\"Instruction:\")\nprint(f\"length: {len(ds['instruction'])}\")\nprint(f\"example: {ds['instruction'][0]} \\n\")\n\nprint(f\"Input:\")\nprint(f\"length: {len(ds['input'])}\")\nprint(f\"example: {ds['input'][0]} \\n\")\n\nprint(f\"Output:\")\nprint(f\"length: {len(ds['output'])}\")\nprint(f\"example: {ds['output'][0]} \\n\")","metadata":{"id":"n89RynbpcHQB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Some plots about the dataset","metadata":{"id":"a9SXu1UFoE4x"}},{"cell_type":"code","source":"instructions = ds['instruction']\ninput_phrases = ds['input']\noutput_phrases = ds['output']","metadata":{"trusted":true,"id":"nLEtHKtISsQ7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the frequency of each unique instruction\ninstruction_counts = {instruction: instructions.count(instruction) for instruction in set(instructions)}\n\n# Sort the instructions by frequency\nsorted_instructions = sorted(instruction_counts.items(), key=lambda x: x[1], reverse=True)\n\n# Separate the instructions and their counts for plotting\nsorted_instruction_names = [item[0] for item in sorted_instructions]\nsorted_instruction_counts = [item[1] for item in sorted_instructions]\n\n# Plotting the frequency of instructions\nplt.figure(figsize=(10, 5))\n\nbars = plt.barh(sorted_instruction_names, sorted_instruction_counts, color='skyblue', edgecolor='black', linewidth=1.2)\nplt.title('Instruction Frequency Distribution')\nplt.xlabel('Frequency')\nplt.ylabel('Instruction')\n\n# Show the plot\nplt.tight_layout()\nplt.show()","metadata":{"id":"4TAMV5DdnRg7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the length of each phrase\ninput_lengths = [len(phrase) for phrase in input_phrases]\noutput_lengths = [len(phrase) for phrase in output_phrases]\n\n# Define the bins for the length ranges\nmax_input = max(input_lengths)\nmax_output = max(output_lengths)\n\ninput_bins = [i * max_input / 10 for i in range(1, 11)]\noutput_bins = [i * max_output / 10 for i in range(1, 11)]\nbin_labels_input = [f'{int(input_bins[i-1])}-{int(input_bins[i])}' for i in range(1, 10)]\nbin_labels_output = [f'{int(output_bins[i-1])}-{int(output_bins[i])}' for i in range(1, 10)]\n\n# Bin the lengths into the categories\ninput_binned = np.digitize(input_lengths, input_bins)  # Categorize based on input lengths\noutput_binned = np.digitize(output_lengths, output_bins)  # Categorize based on output lengths\n\n# Count how many phrases fall into each bin\ninput_bin_counts = [sum(input_binned == i) for i in range(1, len(input_bins))]\noutput_bin_counts = [sum(output_binned == i) for i in range(1, len(output_bins))]\n\n# Plotting the bar charts\nplt.figure(figsize=(20, 10))\n\n# Plotting the input phrase lengths\nplt.subplot(1, 2, 1)\nplt.bar(bin_labels_input, input_bin_counts, color='skyblue', edgecolor='black')\nplt.title('Input Phrases Length Distribution')\nplt.xlabel('Length Range')\nplt.ylabel('Number of Phrases')\n\n# Plotting the output phrase lengths\nplt.subplot(1, 2, 2)\nplt.bar(bin_labels_output, output_bin_counts, color='skyblue', edgecolor='black')\nplt.title('Output Phrases Length Distribution')\nplt.xlabel('Length Range')\nplt.ylabel('Number of Phrases')\n\n# Show the plots\nplt.tight_layout()\nplt.show()","metadata":{"id":"d9lEMELsiqIx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenize","metadata":{"id":"7zqoFYTT2lyy"}},{"cell_type":"code","source":"tokenized_sentences = [re.sub('\\W', ' ', sentence).lower().split() for sentence in input_phrases]\n# remove sentences that are only 1 word long\ntokenized_sentences = [sentence for sentence in tokenized_sentences if len(sentence) > 1]\n\nfor sentence in tokenized_sentences[:5]:\n    print(sentence)","metadata":{"id":"wRH-mf5T2lyy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# merge in & out togheter\n# merged_list = [f\"{a} {b}\" for a, b in zip(input_phrases, output_phrases)]\n\n# remove newline characters\n# docs = [re.sub('\\n', ' ', doc) for doc in merged_list]\n# split sentences\n#sentences = [re.split('[?!.]\\s', doc) for doc in docs]\n#sentences[:3]","metadata":{"id":"yZ8ZCHgj2lyy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from pandas.core.common import flatten\n\n# sentences = list(flatten(sentences))\n# sentences[:20]","metadata":{"id":"yedEUf_32lyy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Word2Vec","metadata":{"id":"f4wIndWS2lyy"}},{"cell_type":"code","source":"model = Word2Vec(tokenized_sentences, vector_size=30, min_count=5, window=10)","metadata":{"id":"e05BkgN-2lyz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = random.sample(list(model.wv.key_to_index), 500)\nword_vectors = model.wv[sample]","metadata":{"id":"blQAYdGr2lyz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3D plot with words","metadata":{"id":"CskZCC7a2lyz"}},{"cell_type":"code","source":"tsne = TSNE(n_components=3, n_iter=2000)\ntsne_embedding = tsne.fit_transform(word_vectors)\n\nx, y, z = np.transpose(tsne_embedding)\n\nfig = px.scatter_3d(x=x[:200],y=y[:200],z=z[:200],text=sample[:200])\nfig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\nfig.show()","metadata":{"id":"_0EGqbEA2lyz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first_question = ['man', 'woman']\n#question = ['rem', 'sleep', 'hallucinations', 'paralysis']\n\nword_vectors = model.wv[first_question+sample]\n\ntsne = TSNE(n_components=3)\ntsne_embedding = tsne.fit_transform(word_vectors)\n\nx, y, z = np.transpose(tsne_embedding)","metadata":{"id":"ruxbKZBt2lyz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"r = (-20,20)\nfig = px.scatter_3d(x=x, y=y, z=z, range_x=r, range_y=r, range_z=r, text=first_question + [None] * 500)\nfig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\nfig.show()","metadata":{"id":"Cem4IiRb2lyz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.wv.most_similar('menopause')","metadata":{"id":"E795I6ej2lyz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vec = model.wv.get_vector('headache') + (model.wv.get_vector('fever') - model.wv.get_vector('drug'))\nmodel.wv.similar_by_vector(vec)","metadata":{"id":"8RkQLdpv2lyz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train and evaluate models","metadata":{"id":"-BMh1vbuc5qp"}},{"cell_type":"markdown","source":"#### Create dataset","metadata":{"id":"VFt5KLR-8w3p"}},{"cell_type":"code","source":"# class MedDataset(Dataset):\n#   def __init__(self, instruction, input, output):\n#     self.instruction = instruction\n#     self.input = input\n#     self.output = output\n\n#   def __len__(self):\n#     return len(self.instruction)\n\n#   def __getitem__(self, idx):\n#     sentence = \"<s>[INST] \"+self.instruction[idx]+\". \"+self.input[idx]+\" [/INST] \"+self.output[idx]+\" </s>\"\n#     return sentence","metadata":{"id":"ND0M0R633VbB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MedDataset(Dataset):\n    def __init__(self, dataset, tokenizer):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        example = self.dataset[idx]\n        messages = [\n            {\"role\": \"system\", \"content\": example['instruction']},\n            {\"role\": \"user\", \"content\": example['input']},\n            {\"role\": \"assistant\", \"content\": example['output']}\n        ]\n\n        prompt = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        tokens = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        )\n\n        tokens['labels'] = tokens['input_ids'].clone()\n        tokens['labels'][tokens['input_ids'] == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": tokens['input_ids'].squeeze(),\n            \"attention_mask\": tokens['attention_mask'].squeeze(),\n            \"labels\": tokens['labels'].squeeze()\n        }","metadata":{"trusted":true,"id":"LacKbkPHSsQ-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"login(token=\"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\")\n\nbase_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\ntokenizer.pad_token = tokenizer.eos_token\n\n# tokenizer.padding_side = \"right\"","metadata":{"trusted":true,"id":"AelKMq1xSsQ-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_dataset = MedDataset(ds, tokenizer)","metadata":{"id":"C6gCSyFM7IsG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset, val_dataset, test_dataset = random_split(tokenized_dataset, [0.8, 0.1, 0.1])\nprint(f\"Train dataset dimension: {len(train_dataset)}\")\nprint(f\"Validation dataset dimension: {len(val_dataset)}\")\nprint(f\"Test dataset dimension: {len(test_dataset)}\")","metadata":{"id":"RRneCjBj9Lag","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_representation=\"nested\"\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=quant_config,\n    device_map={\"\": 0},\n    torch_dtype=torch.float32,\n    trust_remote_code=True\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"id":"gR4BtF8AHD0I","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def convert_to_hf_dataset(med_dataset):\n#     # Create lists to store all formatted text\n#     formatted_texts = []\n\n#     # Iterate through all items in the original dataset\n#     for idx in range(len(med_dataset.instruction)):\n#         # Get the formatted text directly using the dataset's __getitem__\n#         formatted_text = med_dataset[idx]\n#         formatted_texts.append(formatted_text)\n\n#     # Create a dictionary with the required format\n#     dataset_dict = {\n#         'text': formatted_texts\n#     }\n\n#     # Convert to HuggingFace Dataset\n#     hf_dataset = HFDataset.from_dict(dataset_dict)\n\n#     return hf_dataset\n\n# hf_dataset = convert_to_hf_dataset(garnachoDataset)","metadata":{"id":"gzL6yXsGSsQ_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"peft_params = LoraConfig(\n    lora_alpha=32,\n    lora_dropout=0.1,\n    r=16,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"id":"ew98HuwaKW1P","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\ntraining_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    eval_strategy=\"steps\",\n    logging_steps=90,\n    eval_steps = 90,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\",\n    gradient_checkpointing=True\n)","metadata":{"id":"n8dBHX-j-M1J","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    peft_config=peft_params,\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    args=training_params,\n    packing=False,\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model and tokenizer\ntrainer.save_model(\"./fine-tuned-model\")\ntokenizer.save_pretrained(\"./fine-tuned-model\")","metadata":{"trusted":true,"id":"ARnpWrEFSsQ_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.model.save_pretrained(\"model-chatbot-medical-mew\")\ntrainer.tokenizer.save_pretrained(\"model-chatbot-medical-mew\")","metadata":{"id":"awbFBy8v5rDM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test Trained Model","metadata":{"id":"MmC39wTYSsQ_"}},{"cell_type":"markdown","source":"### Load pre-trained model\n\n> Aggiungi citazione\n\n","metadata":{"id":"UwQNV8XLYoLX"}},{"cell_type":"code","source":"trained_model = \"/kaggle/input/medicalllm/pytorch/default/1/model-chatbot-medical-mew\"\nquestion = \"What does low Mobility suggest?\"\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    trained_model, # change with folder where u have the files\n    quantization_config=quant_config,\n    device_map={\"\": 0},\n    torch_dtype=torch.float32,\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(trained_model)","metadata":{"id":"A-XhCQC1YhLP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [{\"role\": \"system\", \"content\": instructions[0]},\n    {\"role\": \"user\", \"content\": question}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\nmodel_inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**model_inputs, max_new_tokens=128)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(f\"Question: {question}\")\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true,"id":"NL-rYUPJSsRD"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Compare with MedLlama","metadata":{}},{"cell_type":"code","source":"# Sostituisci 'medllama-path' con il nome o percorso corretto del modello\ntokenizerMED = AutoTokenizer.from_pretrained(\"johnsnowlabs/JSL-MedLlama-3-8B-v2.0\")\nmodelMED = AutoModelForCausalLM.from_pretrained(\"johnsnowlabs/JSL-MedLlama-3-8B-v2.0\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nltk\nimport nltk\nnltk.download('punkt')\n\nfrom nltk.translate.bleu_score import sentence_bleu\n\ndef calculate_bleu(reference, candidate):\n    \"\"\"\n    Comput BLUE score between the candiate (generate answer) and the reference answer\n    \"\"\"\n    try:\n      score = sentence_bleu(reference, candidate)\n      return score\n    except Exception as e:\n      print(f\"Error during BLUE computing: {e}\")\n      return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"questions = [\"What does low REM sleep latency and experiencing hallucinations/sleep paralysis suggest?\",\n             \"What are some possible causes of low PTH and high calcium levels?\",\n             \"What is the term used to describe a condition of low sodium levels and very high proteins or lipids?\"]\n\ndataset_answer = [\"Low REM sleep latency and experiencing hallucinations/sleep paralysis suggests narcolepsy.\",\n                  \"PTH-independent hypercalcemia, which can be caused by cancer, granulomatous disease, or vitamin D intoxication.\",\n                  \"The term used to describe a condition of low sodium levels and very high proteins or lipids is pseudohyponatremia.\"]\n\nllama_bleu = []\nour_bleu = []\n\nfor i, question in enumerate(questions):\n    print(f\"Question: {question}\")\n    \n    # llama\n    inputs = tokenizerMED(question, return_tensors=\"pt\")\n    outputs = modelMED.generate(inputs[\"input_ids\"], max_length=200, num_return_sequences=1)\n    response = tokenizerMED.decode(outputs[0], skip_special_tokens=True)\n    print(\"Response MedLlama:\", response)\n\n    candidate = response.split()\n    bleu_score = calculate_bleu(dataset_answer[i], candidate)\n\n    if bleu_score is not None:\n        print(f\"BLEU score for Llama answer {i}: {bleu_score}\")   \n        llama_bleu.add(bleu_score)\n\n\nfor i, question in enumerate(questions):\n    # our\n    messages = [{\"role\": \"system\", \"content\": instructions[0]},\n    {\"role\": \"user\", \"content\": question}]\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    model_inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n    outputs = model.generate(**model_inputs, max_new_tokens=128)\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    our = text.split(\"assistant\")[1]\n    print(\"Our Response: \", our)\n\n    candidate = our.split()\n    bleu_score = calculate_bleu(dataset_answer[i], candidate)\n\n    if bleu_score is not None:\n        print(f\"BLEU score for our answer {i}: {bleu_score}\")   \n        our_bleu.add(bleu_score)\n\nimport numpy\nprint(f\"Average BLEU score for our model: {np.median(our_bleu)}\")\nprint(f\"Average BLEU score for Llama model: {np.median(llama_bleu)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Add voice interactivity","metadata":{"id":"kPKlNEM_sFu0"}},{"cell_type":"markdown","source":"### Record Audio","metadata":{"id":"3QSzEjW2MZrc"}},{"cell_type":"code","source":"\"\"\"\nReferences:\nhttps://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/\nhttps://stackoverflow.com/a/18650249\nhttps://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/\nhttps://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/\nhttps://stackoverflow.com/a/49019356\n\"\"\"\n\nAUDIO_HTML = \"\"\"\n<script>\nvar my_div = document.createElement(\"DIV\");\nvar my_p = document.createElement(\"P\");\nvar my_btn = document.createElement(\"BUTTON\");\nvar t = document.createTextNode(\"Press to start recording\");\n\nmy_btn.appendChild(t);\n//my_p.appendChild(my_btn);\nmy_div.appendChild(my_btn);\ndocument.body.appendChild(my_div);\n\nvar base64data = 0;\nvar reader;\nvar recorder, gumStream;\nvar recordButton = my_btn;\n\nvar handleSuccess = function(stream) {\n  gumStream = stream;\n  var options = {\n    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n    mimeType : 'audio/webm;codecs=opus'\n    //mimeType : 'audio/webm;codecs=pcm'\n  };\n  //recorder = new MediaRecorder(stream, options);\n  recorder = new MediaRecorder(stream);\n  recorder.ondataavailable = function(e) {\n    var url = URL.createObjectURL(e.data);\n    var preview = document.createElement('audio');\n    preview.controls = true;\n    preview.src = url;\n    document.body.appendChild(preview);\n\n    reader = new FileReader();\n    reader.readAsDataURL(e.data);\n    reader.onloadend = function() {\n      base64data = reader.result;\n      //console.log(\"Inside FileReader:\" + base64data);\n    }\n  };\n  recorder.start();\n  };\n\nrecordButton.innerText = \"Recording... press to stop\";\n\nnavigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n\n\nfunction toggleRecording() {\n  if (recorder && recorder.state == \"recording\") {\n      recorder.stop();\n      gumStream.getAudioTracks()[0].stop();\n      recordButton.innerText = \"Saving the recording... pls wait!\"\n  }\n}\n\n// https://stackoverflow.com/a/951057\nfunction sleep(ms) {\n  return new Promise(resolve => setTimeout(resolve, ms));\n}\n\nvar data = new Promise(resolve=>{\n//recordButton.addEventListener(\"click\", toggleRecording);\nrecordButton.onclick = ()=>{\ntoggleRecording()\n\nsleep(2000).then(() => {\n  // wait 2000ms for the data to be available...\n  // ideally this should use something like await...\n  //console.log(\"Inside data:\" + base64data)\n  resolve(base64data.toString())\n\n});\n\n}\n});\n\n</script>\n\"\"\"\n\ndef get_audio():\n  display(HTML(AUDIO_HTML))\n  data = eval_js(\"data\")\n  binary = b64decode(data.split(',')[1])\n\n  process = (ffmpeg\n    .input('pipe:0')\n    .output('pipe:1', format='wav')\n    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n  )\n  output, err = process.communicate(input=binary)\n\n  riff_chunk_size = len(output) - 8\n  # Break up the chunk size into four bytes, held in b.\n  q = riff_chunk_size\n  b = []\n  for i in range(4):\n      q, r = divmod(q, 256)\n      b.append(r)\n\n  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n  riff = output[:4] + bytes(b) + output[8:]\n\n  sr, audio = wav_read(io.BytesIO(riff))\n\n  return audio, sr","metadata":{"id":"7uLKcUMQMeaL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"audio, sr = get_audio()","metadata":{"id":"oo1ink-uMiYo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scipy.io.wavfile.write('./recording.wav', sr, audio)","metadata":{"id":"DaV9sVvZMlCL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Speech to Text","metadata":{"id":"_1k0fMEMNtl2"}},{"cell_type":"code","source":"whis = whisper.load_model(\"base\")\n\n# load audio and pad/trim it to fit 30 seconds\naudio = whisper.load_audio(\"./recording.wav\")\naudio = whisper.pad_or_trim(audio)\n\n# load audio and pad/trim it to fit 30 seconds\nfig = plt.figure(figsize=(16,4))\nplt.plot(audio, linewidth=0.4)\nplt.ylabel('Amplitude')\nplt.xlabel('Samples')\nplt.show()\n\n# make log-Mel spectrogram and move to the same device as the model\nmel = whisper.log_mel_spectrogram(audio).to(whis.device)\n\n# Visualize spectrogram\nfig = plt.figure(figsize=(10,4))\nplt.pcolormesh(mel.cpu().numpy())\nplt.colorbar(label='Power [dB]')\nplt.ylabel('Frequency [Hz]')\nplt.xlabel('Time [10ms]')\nplt.show()\n\n# Use the mel spectrogram to detect the language\n_, probs = whis.detect_language(mel)\nlang = max(probs, key=probs.get)\n\n# Print result\nprint(f\"Detected language: {lang}, confidence: {probs[lang]}\")\n\n# decode the audio\noptions = whisper.DecodingOptions(fp16 = False)\nresult = whisper.decode(whis, mel, options)\n\n# print the recognized text\nprint(result.text)","metadata":{"id":"sPQmD992Lpjh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dump result text into model\ninputs = tokenizer(result.text, return_tensors=\"pt\")\ninputs = {k: v.to(model.device) for k, v in inputs.items()}\noutput = model.generate(**inputs)","metadata":{"id":"La6g7PIIN3yo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Text to Speech","metadata":{"id":"LZLo43qaNyqn"}},{"cell_type":"code","source":"tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\ntacotron2 = tacotron2.to('cuda')\ntacotron2.eval()","metadata":{"id":"DVEewwceOF-Y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp16')\nwaveglow = waveglow.remove_weightnorm(waveglow)\nwaveglow = waveglow.to('cuda')\nwaveglow.eval()","metadata":{"id":"tcKqPWGXOJz9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add model response\nprint(tokenizer.decode(output[0]))\ntext = tokenizer.decode(output[0])","metadata":{"id":"DlHbL8_YOMON","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')\nsequences, lengths = utils.prepare_input_sequence([text])\nsequences","metadata":{"id":"SYGrvo4YOOBX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    mel, _, _ = tacotron2.infer(sequences, lengths)\n\n%matplotlib inline\n\nfig = plt.figure(figsize=(10,4))\nplt.pcolormesh(mel[0].cpu().numpy())\nplt.colorbar(label='Power [dB]')\nplt.ylabel('Frequency [Hz]')\nplt.xlabel('Time [10ms]')\nplt.show()","metadata":{"id":"oOyIqRq9OQJP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    audio = waveglow.infer(mel)\naudio_numpy = audio[0].data.cpu().numpy()\nrate = 22050\n\nfig = plt.figure(figsize=(16,4))\nplt.plot(audio_numpy, linewidth=0.4)\nplt.ylabel('Amplitude')\nplt.xlabel('Samples')\nplt.show()","metadata":{"id":"t-dE1l-dOVwn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Audio(audio_numpy, rate=rate)","metadata":{"id":"PNwE-1SuObFF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Potential extensions","metadata":{"id":"s22bSn7_sFu6"}},{"cell_type":"code","source":"","metadata":{"id":"h1mPCVNJdGSr","trusted":true},"outputs":[],"execution_count":null}]}