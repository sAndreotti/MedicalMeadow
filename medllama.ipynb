{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"include_colab_link":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":218074,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":185968,"modelId":208088}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/sAndreotti/MedicalMeadow/blob/main/ATML_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text","editable":false}},{"cell_type":"markdown","source":"Import Libraries","metadata":{"id":"29nxqgohSsQ3","editable":false}},{"cell_type":"code","source":"!pip install datasets accelerate peft bitsandbytes transformers trl==0.12.0 plotly huggingface_hub\n!pip install --upgrade smart_open\n!pip install --upgrade gensim\n!pip install ffmpeg-python\n!pip install -U openai-whisper\n!pip install scipy librosa unidecode inflect","metadata":{"id":"SAho3HGib9-U","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\nfrom trl import SFTTrainer\nimport re\nfrom gensim.models.word2vec import Word2Vec\nimport plotly.express as px\nimport random\nfrom sklearn.manifold import TSNE\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import random_split\nfrom peft import prepare_model_for_kbit_training, LoraConfig\nfrom huggingface_hub import login\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments\n)\nfrom datasets import Dataset as HFDataset\nfrom peft import AutoPeftModelForCausalLM\nimport whisper\nfrom IPython.display import Audio","metadata":{"id":"q_JimYqjjY4S","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Import Libraries for audio part","metadata":{"id":"E2DDMWKGSsQ5","editable":false}},{"cell_type":"code","source":"from IPython.display import HTML, Audio\nfrom google.colab.output import eval_js\nfrom base64 import b64decode\nfrom scipy.io.wavfile import read as wav_read\nimport io\nimport ffmpeg\nimport scipy","metadata":{"id":"lkjSoQT4SsQ6","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Investigate Dataset","metadata":{"id":"XKL46PcIc21t","editable":false}},{"cell_type":"code","source":"ds = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\nds = ds['train']\nds","metadata":{"id":"YltBm7i4b0IM","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ds.features, \"\\n\")\nprint(\"Instruction:\")\nprint(f\"length: {len(ds['instruction'])}\")\nprint(f\"example: {ds['instruction'][0]} \\n\")\n\nprint(f\"Input:\")\nprint(f\"length: {len(ds['input'])}\")\nprint(f\"example: {ds['input'][0]} \\n\")\n\nprint(f\"Output:\")\nprint(f\"length: {len(ds['output'])}\")\nprint(f\"example: {ds['output'][0]} \\n\")","metadata":{"id":"n89RynbpcHQB","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Some plots about the dataset","metadata":{"id":"a9SXu1UFoE4x","editable":false}},{"cell_type":"code","source":"instructions = ds['instruction']\ninput_phrases = ds['input']\noutput_phrases = ds['output']","metadata":{"trusted":true,"id":"nLEtHKtISsQ7","editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the frequency of each unique instruction\ninstruction_counts = {instruction: instructions.count(instruction) for instruction in set(instructions)}\n\n# Sort the instructions by frequency\nsorted_instructions = sorted(instruction_counts.items(), key=lambda x: x[1], reverse=True)\n\n# Separate the instructions and their counts for plotting\nsorted_instruction_names = [item[0] for item in sorted_instructions]\nsorted_instruction_counts = [item[1] for item in sorted_instructions]\n\n# Plotting the frequency of instructions\nplt.figure(figsize=(10, 5))\n\nbars = plt.barh(sorted_instruction_names, sorted_instruction_counts, color='skyblue', edgecolor='black', linewidth=1.2)\nplt.title('Instruction Frequency Distribution')\nplt.xlabel('Frequency')\nplt.ylabel('Instruction')\n\n# Show the plot\nplt.tight_layout()\nplt.show()","metadata":{"id":"4TAMV5DdnRg7","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the length of each phrase\ninput_lengths = [len(phrase) for phrase in input_phrases]\noutput_lengths = [len(phrase) for phrase in output_phrases]\n\n# Define the bins for the length ranges\nmax_input = max(input_lengths)\nmax_output = max(output_lengths)\n\ninput_bins = [i * max_input / 10 for i in range(1, 11)]\noutput_bins = [i * max_output / 10 for i in range(1, 11)]\nbin_labels_input = [f'{int(input_bins[i-1])}-{int(input_bins[i])}' for i in range(1, 10)]\nbin_labels_output = [f'{int(output_bins[i-1])}-{int(output_bins[i])}' for i in range(1, 10)]\n\n# Bin the lengths into the categories\ninput_binned = np.digitize(input_lengths, input_bins)  # Categorize based on input lengths\noutput_binned = np.digitize(output_lengths, output_bins)  # Categorize based on output lengths\n\n# Count how many phrases fall into each bin\ninput_bin_counts = [sum(input_binned == i) for i in range(1, len(input_bins))]\noutput_bin_counts = [sum(output_binned == i) for i in range(1, len(output_bins))]\n\n# Plotting the bar charts\nplt.figure(figsize=(20, 10))\n\n# Plotting the input phrase lengths\nplt.subplot(1, 2, 1)\nplt.bar(bin_labels_input, input_bin_counts, color='skyblue', edgecolor='black')\nplt.title('Input Phrases Length Distribution')\nplt.xlabel('Length Range')\nplt.ylabel('Number of Phrases')\n\n# Plotting the output phrase lengths\nplt.subplot(1, 2, 2)\nplt.bar(bin_labels_output, output_bin_counts, color='skyblue', edgecolor='black')\nplt.title('Output Phrases Length Distribution')\nplt.xlabel('Length Range')\nplt.ylabel('Number of Phrases')\n\n# Show the plots\nplt.tight_layout()\nplt.show()","metadata":{"id":"d9lEMELsiqIx","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train and evaluate models","metadata":{"id":"-BMh1vbuc5qp","editable":false}},{"cell_type":"markdown","source":"#### Create dataset","metadata":{"id":"VFt5KLR-8w3p","editable":false}},{"cell_type":"code","source":"# class MedDataset(Dataset):\n#   def __init__(self, instruction, input, output):\n#     self.instruction = instruction\n#     self.input = input\n#     self.output = output\n\n#   def __len__(self):\n#     return len(self.instruction)\n\n#   def __getitem__(self, idx):\n#     sentence = \"<s>[INST] \"+self.instruction[idx]+\". \"+self.input[idx]+\" [/INST] \"+self.output[idx]+\" </s>\"\n#     return sentence","metadata":{"id":"ND0M0R633VbB","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MedDataset(Dataset):\n    def __init__(self, dataset, tokenizer):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        example = self.dataset[idx]\n        messages = [\n            {\"role\": \"system\", \"content\": example['instruction']},\n            {\"role\": \"user\", \"content\": example['input']},\n            {\"role\": \"assistant\", \"content\": example['output']}\n        ]\n\n        prompt = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        tokens = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        )\n\n        tokens['labels'] = tokens['input_ids'].clone()\n        tokens['labels'][tokens['input_ids'] == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": tokens['input_ids'].squeeze(),\n            \"attention_mask\": tokens['attention_mask'].squeeze(),\n            \"labels\": tokens['labels'].squeeze()\n        }","metadata":{"trusted":true,"id":"LacKbkPHSsQ-","editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"login(token=\"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\")\n\nbase_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\ntokenizer.pad_token = tokenizer.eos_token\n\n# tokenizer.padding_side = \"right\"","metadata":{"trusted":true,"id":"AelKMq1xSsQ-","editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_dataset = MedDataset(ds, tokenizer)","metadata":{"id":"C6gCSyFM7IsG","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset, val_dataset, test_dataset = random_split(tokenized_dataset, [0.8, 0.1, 0.1])\nprint(f\"Train dataset dimension: {len(train_dataset)}\")\nprint(f\"Validation dataset dimension: {len(val_dataset)}\")\nprint(f\"Test dataset dimension: {len(test_dataset)}\")","metadata":{"id":"RRneCjBj9Lag","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_representation=\"nested\"\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=quant_config,\n    device_map={\"\": 0},\n    torch_dtype=torch.float32,\n    trust_remote_code=True\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"id":"gR4BtF8AHD0I","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def convert_to_hf_dataset(med_dataset):\n#     # Create lists to store all formatted text\n#     formatted_texts = []\n\n#     # Iterate through all items in the original dataset\n#     for idx in range(len(med_dataset.instruction)):\n#         # Get the formatted text directly using the dataset's __getitem__\n#         formatted_text = med_dataset[idx]\n#         formatted_texts.append(formatted_text)\n\n#     # Create a dictionary with the required format\n#     dataset_dict = {\n#         'text': formatted_texts\n#     }\n\n#     # Convert to HuggingFace Dataset\n#     hf_dataset = HFDataset.from_dict(dataset_dict)\n\n#     return hf_dataset\n\n# hf_dataset = convert_to_hf_dataset(garnachoDataset)","metadata":{"id":"gzL6yXsGSsQ_","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"peft_params = LoraConfig(\n    lora_alpha=32,\n    lora_dropout=0.1,\n    r=16,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"id":"ew98HuwaKW1P","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\ntraining_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    eval_strategy=\"steps\",\n    logging_steps=90,\n    eval_steps = 90,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\",\n    gradient_checkpointing=True\n)","metadata":{"id":"n8dBHX-j-M1J","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainer = SFTTrainer(\n#     model=model,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,\n#     peft_config=peft_params,\n#     max_seq_length=256,\n#     tokenizer=tokenizer,\n#     args=training_params,\n#     packing=False,\n# )\n\n# # Train the model\n# trainer.train()\n\n# # Save the model and tokenizer\n# trainer.save_model(\"./fine-tuned-model\")\n# tokenizer.save_pretrained(\"./fine-tuned-model\")","metadata":{"trusted":true,"id":"ARnpWrEFSsQ_","editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainer.model.save_pretrained(\"model-chatbot-medical-mew\")\n# trainer.tokenizer.save_pretrained(\"model-chatbot-medical-mew\")","metadata":{"id":"awbFBy8v5rDM","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test Trained Model","metadata":{"id":"MmC39wTYSsQ_","editable":false}},{"cell_type":"markdown","source":"### Load pre-trained model","metadata":{"id":"UwQNV8XLYoLX","editable":false}},{"cell_type":"code","source":"trained_model = \"/kaggle/input/medicalllm/pytorch/default/1/model-chatbot-medical-mew\"\nquestion = \"What does low Mobility suggest?\"\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    trained_model, # change with folder where u have the files\n    quantization_config=quant_config,\n    device_map={\"\": 0},\n    torch_dtype=torch.float32,\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(trained_model)","metadata":{"id":"A-XhCQC1YhLP","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [{\"role\": \"system\", \"content\": instructions[0]},\n    {\"role\": \"user\", \"content\": question}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\nmodel_inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**model_inputs, max_new_tokens=128)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(f\"Question: {question}\")\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true,"id":"NL-rYUPJSsRD","editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Compare with MedLlama","metadata":{"editable":false}},{"cell_type":"code","source":"# tokenizerMED = AutoTokenizer.from_pretrained(\"OpenScienceReseach/Med-LLaMA-7b\")\n# modelMED = AutoModelForCausalLM.from_pretrained(\"OpenScienceReseach/Med-LLaMA-7b\") funzia ma lento\n\ntokenizerMED = AutoTokenizer.from_pretrained(\"medalpaca/medalpaca-7b\")\nmodelMED = AutoModelForCausalLM.from_pretrained(\"medalpaca/medalpaca-7b\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nltk\nimport nltk\nnltk.download('punkt')\n\nfrom nltk.translate.bleu_score import sentence_bleu\n\ndef calculate_bleu(reference, candidate):\n    \"\"\"\n    Comput BLUE score between the candiate (generate answer) and the reference answer\n    \"\"\"\n    try:\n      score = sentence_bleu(reference, candidate)\n      return score\n    except Exception as e:\n      print(f\"Error during BLUE computing: {e}\")\n      return None","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"questions = [\"What does low REM sleep latency and experiencing hallucinations/sleep paralysis suggest?\",\n             \"What are some possible causes of low PTH and high calcium levels?\",\n             \"What is the term used to describe a condition of low sodium levels and very high proteins or lipids?\"]\n\ndataset_answer = [\"Low REM sleep latency and experiencing hallucinations/sleep paralysis suggests narcolepsy.\",\n                  \"PTH-independent hypercalcemia, which can be caused by cancer, granulomatous disease, or vitamin D intoxication.\",\n                  \"The term used to describe a condition of low sodium levels and very high proteins or lipids is pseudohyponatremia.\"]\n\nllama_bleu = []\nour_bleu = []\n\nfrom transformers import pipeline\nimport time\n\nfor i, question in enumerate(questions):\n    print(f\"Question: {question}\")\n    start = time.time()    \n    # llama\n    pl = pipeline(\"text-generation\", model=modelMED, tokenizer=tokenizerMED)\n    #pl.to('cuda')\n    response = pl(question)\n    end = time.time() - start\n    print(\"time for generate response: \", end)\n    print(\"Response MedLlama:\", response)\n\n    candidate = response.split()\n    bleu_score = calculate_bleu(dataset_answer[i], candidate)\n\n    if bleu_score is not None:\n        print(f\"BLEU score for Llama answer {i}: {bleu_score}\")   \n        llama_bleu.append(bleu_score)\n\n\nfor i, question in enumerate(questions):\n    # our\n    messages = [{\"role\": \"system\", \"content\": instructions[0]},\n    {\"role\": \"user\", \"content\": question}]\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    model_inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n    outputs = model.generate(**model_inputs, max_new_tokens=128)\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    our = text.split(\"assistant\")[1]\n    print(\"Our Response: \", our)\n\n    candidate = our.split()\n    bleu_score = calculate_bleu(dataset_answer[i], candidate)\n\n    if bleu_score is not None:\n        print(f\"BLEU score for our answer {i}: {bleu_score}\")   \n        our_bleu.append(bleu_score)\n\nimport numpy\nprint(f\"Average BLEU score for our model: {np.median(our_bleu)}\")\nprint(f\"Average BLEU score for Llama model: {np.median(llama_bleu)}\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}